<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>garret - dev-issues</title><link href="https://imjang57.github.io/garret/" rel="alternate"></link><link href="https://imjang57.github.io/garret/feeds/dev-issues.atom.xml" rel="self"></link><id>https://imjang57.github.io/garret/</id><updated>2017-01-10T00:00:00+09:00</updated><entry><title>Windows 에서 apache zeppelin 사용 시 HiveContext 에러</title><link href="https://imjang57.github.io/garret/hive-context-error-of-zeppelin-on-windows.html" rel="alternate"></link><published>2017-01-10T00:00:00+09:00</published><updated>2016-01-10T00:00:00+09:00</updated><author><name>imjang57</name></author><id>tag:imjang57.github.io,2017-01-10:/garret/hive-context-error-of-zeppelin-on-windows.html</id><summary type="html">&lt;p&gt;Windows 에서 apache zeppelin 사용 시 HiveContext 에러가 발생하는 경우&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Windows 에서 zeppelin 실행&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://imjang57.github.io/garret/run-spark-shell-on-windows.html"&gt;윈도우에서 spark-shell을 실행하는 방법&lt;/a&gt; 에서 윈도우에서 spark-shell 을 실행하는 방법에 대해 알아봤다. spark-shell 이 실행가능하니까 당연히 zeppelin 도 실행가능할거라고 생각된다. 그래서 실행해보니 에러가 발생했다..-_-..; 메시지가 매우 긴데.. 다음과 같은 메시지가 나오는 경우였다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:C:/Users/imjan/Desktop/zeppelin-0.6.2/spark-warehouse
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;%ZEPPELIN_HOME%\spark-warehouse&lt;/code&gt; 를 hive 테이블들을 저장하기 위해 사용하는 것 같다. 그래서 &lt;code&gt;%ZEPPELIN_HOME%\spark-warehouse&lt;/code&gt; 폴더를 생성하고 &lt;a href="https://imjang57.github.io/garret/run-spark-shell-on-windows.html"&gt;윈도우에서 spark-shell을 실행하는 방법&lt;/a&gt; 포스트에서 했떤 것 처럼 winutils 를 사용해서 권한을 설정해줬다&lt;/p&gt;
&lt;p&gt;그런데 같은 에러가 또 발생한다..!!!!..ㅆ....&lt;/p&gt;
&lt;p&gt;그래서 찾다보니 &lt;a href="https://hernandezpaul.wordpress.com/2016/11/14/apache-zeppelin-installation-on-windows-10/"&gt;이 블로그 포스트&lt;/a&gt; 에서 해결책을 찾았다.&lt;/p&gt;
&lt;p&gt;결론은 &lt;code&gt;spark.sql.warehouse.dir&lt;/code&gt; 이라는 설정값을 정확하게 설정해줘야 한다는 것이다. &lt;code&gt;%ZEPPELIN_HOME%\conf\interpreter.json&lt;/code&gt; 을 열어서 spark interprefter 설정을 찾아서 아래와 같이 &lt;code&gt;spark.sql.warehouse.dir&lt;/code&gt; 을 추가하자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="err"&gt;...&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;2C5DV85NF&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;2C5DV85NF&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;spark&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;group&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;spark&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;properties&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;spark.sql.warehouse.dir&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;file:///C:/Users/imjan/Desktop/zeppelin-0.6.2/spark-warehouse&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;spark.executor.memory&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;args&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.spark.printREPLOutput&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;spark.cores.max&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.dep.additionalRemoteRepository&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;spark-packages,http://dl.bintray.com/spark-packages/maven,false;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.spark.sql.stacktrace&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;false&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.spark.importImplicit&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.spark.concurrentSQL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;false&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.spark.useHiveContext&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.pyspark.python&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;python&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.dep.localrepo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;local-repo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.R.knitr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.spark.maxResult&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1000&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;master&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;local[*]&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;spark.app.name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Zeppelin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.R.image.width&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;100%&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.R.render.options&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;out.format \u003d \u0027html\u0027, comment \u003d NA, echo \u003d FALSE, results \u003d \u0027asis\u0027, message \u003d F, warning \u003d F&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;zeppelin.R.cmd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;R&amp;quot;&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="err"&gt;...&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="err"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;그리고 아래 코드를 실행해서 테스트해보니 잘된다. &lt;code&gt;%ZEPPELIN_HOME&lt;/code&gt;\spark-warehouse&lt;code&gt;에&lt;/code&gt;tmptable` 이라는 폴더가 생성된 걸 확인할 수 있다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;hiveCtx&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apache&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;HiveContext&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;hiveCtx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;select 1, 2&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saveAsTable&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;tmpTable&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;p&gt;위의 내용은 zeppelin 내장 spark interpreter 를 사용한 경우라서 사용 형태라 다른 경우 맞지 않을 수도 있다.&lt;/p&gt;
&lt;h1&gt;참고&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://hernandezpaul.wordpress.com/2016/11/14/apache-zeppelin-installation-on-windows-10/"&gt;Apache Zeppelin installation on Windows 10&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="spark"></category><category term="zeppelin"></category></entry><entry><title>Windows 에서 spark-shell 을 실행하는 방법</title><link href="https://imjang57.github.io/garret/run-spark-shell-on-windows.html" rel="alternate"></link><published>2017-01-10T00:00:00+09:00</published><updated>2016-01-10T00:00:00+09:00</updated><author><name>imjang57</name></author><id>tag:imjang57.github.io,2017-01-10:/garret/run-spark-shell-on-windows.html</id><summary type="html">&lt;p&gt;Windows 에서 spark-shell 을 실행하는 방법&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Windows 에서 spark-shell 실행&lt;/h1&gt;
&lt;p&gt;윈도우에서 spark-shell 실행 시 NullPointerException 발생할 경우 문제 해결하는 방법에 대한 글이다.&lt;/p&gt;
&lt;h1&gt;윈도우에서 Spark 실행 시 RuntimeException(NullPointerException) 발생&lt;/h1&gt;
&lt;p&gt;Spark 은 보통 리눅스에서 사용되지만 Local mode 로 실행하면 윈도우에서도 실행할 수 있다. 그런데 리눅스에서는 단순히 spark-shell 스크립트를 실행하면 되는데 윈도우에서는 아래와 같은 에러가 발생하는 경우가 있다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;java.lang.RuntimeException: java.lang.NullPointerException&lt;/span&gt;
&lt;span class="x"&gt;        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)&lt;/span&gt;
&lt;span class="x"&gt;at org.apache.spark.sql.hive.client.ClientWrapper.&amp;lt;init&amp;gt;(ClientWrapper.scala:194)&lt;/span&gt;
&lt;span class="x"&gt;at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:238)&lt;/span&gt;
&lt;span class="x"&gt;at org.apache.spark.sql.hive.HiveContext.executionHive&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;lzycompute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;HiveContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="m"&gt;218&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt;at org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:208)&lt;/span&gt;
&lt;span class="x"&gt;at org.apache.spark.sql.hive.HiveContext.functionRegistry&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;lzycompute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;HiveContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="m"&gt;462&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt;at org.apache.spark.sql.hive.HiveContext.functionRegistry(HiveContext.scala:461)&lt;/span&gt;
&lt;span class="x"&gt;at org.apache.spark.sql.UDFRegistration.&amp;lt;init&amp;gt;(UDFRegistration.scala:40)&lt;/span&gt;
&lt;span class="x"&gt;................&lt;/span&gt;
&lt;span class="x"&gt;................&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;그냥 NullPointerException 이고 아무런 메시지가 없다…-_-…&lt;/p&gt;
&lt;p&gt;내 기억에 작년에 분명 spark-shell 을 윈도우에서 잘 썼었는데 갑자기 에러가 나서 당황했는데, HiveContext 가 원인이었다.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blogs.msdn.microsoft.com/arsen/2016/02/09/resolving-spark-1-6-0-java-lang-nullpointerexception-not-found-value-sqlcontext-error-when-running-spark-shell-on-windows-10-64-bit/"&gt;MSDN 블로그의 한 글&lt;/a&gt;에 따르면 윈도우에서 Spark 이 HiveContext 를 초기화하기 위해서는 &lt;code&gt;winutils.exe&lt;/code&gt; 이라는 파일이 필요하다. 블로그에 따르면 HiveContext 를 초기화하는데 Hadoop 의 Native libraries 를 필요로 하기 때문이라는 듯 하다.&lt;/p&gt;
&lt;p&gt;이번에 사용하려고 했던 spark 빌드는 hive 를 포함한 빌드이기 때문에 spark-shell 이 실행될 때 무조건 HiveContext 를 초기화하게 되어 있었다. 생각해보니 작년에 사용했던 건 hive 를 제외하고 소스를 빌드해서 사용했던 것 같다.ㅠㅠ&lt;/p&gt;
&lt;p&gt;어쨌든 이 문제를 해결하기 위해서는 아래와 같은 것들이 필요하다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;winutils.exe 다운로드 및 %HADOOP_HOME%\bin 에 복사&lt;/li&gt;
&lt;li&gt;HADOOP_HOME 환경 변수 설정&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;자세한 내용은 블로그에 쉽게 나와있으니 읽어보면 되고, 아래 명령들만 기억하자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;%HADOOP_HOME%&lt;span class="se"&gt;\b&lt;/span&gt;in&lt;span class="se"&gt;\w&lt;/span&gt;inutils.exe ls &lt;span class="se"&gt;\t&lt;/span&gt;mp&lt;span class="se"&gt;\h&lt;/span&gt;ive
%HADOOP_HOME%&lt;span class="se"&gt;\b&lt;/span&gt;in&lt;span class="se"&gt;\w&lt;/span&gt;inutils.exe chmod &lt;span class="m"&gt;777&lt;/span&gt; &lt;span class="se"&gt;\t&lt;/span&gt;mp&lt;span class="se"&gt;\h&lt;/span&gt;ive
%HADOOP_HOME%&lt;span class="se"&gt;\b&lt;/span&gt;in&lt;span class="se"&gt;\w&lt;/span&gt;inutils.exe ls &lt;span class="se"&gt;\t&lt;/span&gt;mp&lt;span class="se"&gt;\h&lt;/span&gt;ive
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;hadoop 은 설치하고 파일 옮기고 귀찮으니까 그냥 &lt;a href="https://github.com/steveloughran/winutils"&gt;winutils github&lt;/a&gt; 에서 다운로드 하자. 윈도우에서 hadoop 을 잘 쓸 수 있게 &lt;code&gt;winutils.exe&lt;/code&gt; 파일까지 포함시켜서 구성되어 있다. 다운로드 해서 &lt;code&gt;%HADOOP_HOME%&lt;/code&gt; 만 잘 잡아주면 된다.&lt;/p&gt;
&lt;h1&gt;추가로 발생한 문제&lt;/h1&gt;
&lt;p&gt;나의 경우에 위 내용을 다 했는데도 에러가 발생했다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;java.lang.RuntimeException: java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwx------&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.sql.hive.client.ClientWrapper.&amp;lt;init&amp;gt;(ClientWrapper.scala:204)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:238)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.sql.hive.HiveContext.executionHive&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;lzycompute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;HiveContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="m"&gt;218&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:208)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.sql.hive.HiveContext.functionRegistry&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;lzycompute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;HiveContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="m"&gt;462&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.sql.hive.HiveContext.functionRegistry(HiveContext.scala:461)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.sql.UDFRegistration.&amp;lt;init&amp;gt;(UDFRegistration.scala:40)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.sql.SQLContext.&amp;lt;init&amp;gt;(SQLContext.scala:330)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.sql.hive.HiveContext.&amp;lt;init&amp;gt;(HiveContext.scala:97)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.sql.hive.HiveContext.&amp;lt;init&amp;gt;(HiveContext.scala:101)&lt;/span&gt;
&lt;span class="x"&gt;  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;/span&gt;
&lt;span class="x"&gt;  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&lt;/span&gt;
&lt;span class="x"&gt;  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;/span&gt;
&lt;span class="x"&gt;  at java.lang.reflect.Constructor.newInstance(Constructor.java:422)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.spark.repl.Main&lt;/span&gt;&lt;span class="p"&gt;$.&lt;/span&gt;&lt;span class="nv"&gt;createSQLContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;Main.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="m"&gt;89&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt;  ... 47 elided&lt;/span&gt;
&lt;span class="x"&gt;Caused by: java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwx------&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:612)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:554)&lt;/span&gt;
&lt;span class="x"&gt;  at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:508)&lt;/span&gt;
&lt;span class="x"&gt;  ... 62 more&lt;/span&gt;
&lt;span class="x"&gt;&amp;lt;console&amp;gt;:13: error: not found: value sqlContext&lt;/span&gt;
&lt;span class="x"&gt;       import sqlContext.implicits._&lt;/span&gt;
&lt;span class="x"&gt;              ^&lt;/span&gt;
&lt;span class="x"&gt;&amp;lt;console&amp;gt;:13: error: not found: value sqlContext&lt;/span&gt;
&lt;span class="x"&gt;       import sqlContext.sql&lt;/span&gt;
&lt;span class="x"&gt;              ^&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;이번에는 그래도 쉽게 이유를 알 수 있었다. 에러 메시지를 보면 다음과 같다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwx------
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;즉, 권한 문제다. 그런데 원래 위의 내용대로 하면 위 문제가 해결되어야 한다. 뭐가 문제인가.&lt;/p&gt;
&lt;p&gt;문제는 내가 C 드라이브가 아니라 E 드라이브에서 spark-shell 을 실행해서 였다. 위의 내용대로 해서 &lt;code&gt;C:\tmp\hive&lt;/code&gt; 를 생성하고 권한 설정했는데, spark-shell 은 E 드라이브에서 실행해서 실제로는 &lt;code&gt;E:\tmp\hive&lt;/code&gt; 에 접근했던 거였다. 그래서 &lt;code&gt;E:\tmp\hive&lt;/code&gt; 로 다시 권한을 설정하고 실행하니 잘 되었다.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;code&gt;HADOOP_HOME&lt;/code&gt; 을 환경변수로 해주기 귀찮아서 더 해보니까 conf/spark-env.cmd 파일에 아래처럼 추가해줘도 동작한다.&lt;/p&gt;
&lt;p&gt;set HADOOP_HOME=C:\hadoop\hadoop-2.6.4&lt;/p&gt;
&lt;h1&gt;참고&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blogs.msdn.microsoft.com/arsen/2016/02/09/resolving-spark-1-6-0-java-lang-nullpointerexception-not-found-value-sqlcontext-error-when-running-spark-shell-on-windows-10-64-bit/"&gt;Resolving Spark 1.6.0 "java.lang.NullPointerException, not found: value sqlContext" error when running spark-shell on Windows 10 (64-bit)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/steveloughran/winutils/tree/master/hadoop-2.6.4/bin"&gt;hadoop 2.6.4 winutils github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="spark"></category></entry><entry><title>Java version 을 체크하는 bash script</title><link href="https://imjang57.github.io/garret/bash-script-checking-java-version.html" rel="alternate"></link><published>2016-12-30T00:00:00+09:00</published><updated>2016-12-30T00:00:00+09:00</updated><author><name>imjang57</name></author><id>tag:imjang57.github.io,2016-12-30:/garret/bash-script-checking-java-version.html</id><summary type="html">&lt;p&gt;Java version 을 체크하기 위한 bash script&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Check Java version from bash script&lt;/h1&gt;
&lt;p&gt;리눅스에서 자바 버전을 체크하기 위한 꼼수 스크립트를 간단하게 작성해서 저장하기 위한 글입니다.&lt;/p&gt;
&lt;p&gt;java -version 명령의 결과:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ java -version
java version &lt;span class="s2"&gt;&amp;quot;1.8.0_73&amp;quot;&lt;/span&gt;
Java&lt;span class="o"&gt;(&lt;/span&gt;TM&lt;span class="o"&gt;)&lt;/span&gt; SE Runtime Environment &lt;span class="o"&gt;(&lt;/span&gt;build 1.8.0_73-b02&lt;span class="o"&gt;)&lt;/span&gt;
Java HotSpot&lt;span class="o"&gt;(&lt;/span&gt;TM&lt;span class="o"&gt;)&lt;/span&gt; 64-Bit Server VM &lt;span class="o"&gt;(&lt;/span&gt;build 25.73-b02, mixed mode&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;환경변수 $JAVA_HOME을 찾아서 하는 bash script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$JAVA_HOME&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
  &lt;span class="nv"&gt;JAVA_CMD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$JAVA_HOME&lt;/span&gt;&lt;span class="s2"&gt;/bin/java&amp;quot;&lt;/span&gt;
  &lt;span class="nv"&gt;JAVA_VERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nv"&gt;$JAVA_CMD&lt;/span&gt; -version 2&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; awk -F&lt;span class="s1"&gt;&amp;#39;&amp;quot;&amp;#39;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/version/ {print $2}&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; awk -F&lt;span class="s1"&gt;&amp;#39;_&amp;#39;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{print $1}&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
  &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Run using java version &lt;/span&gt;&lt;span class="nv"&gt;$JAVA_VERSION&lt;/span&gt;&lt;span class="s2"&gt; (JAVA_HOME is &lt;/span&gt;&lt;span class="nv"&gt;$JAVA_HOME&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt;
  &lt;span class="o"&gt;[[&lt;/span&gt; &lt;span class="nv"&gt;$JAVA_VERSION&lt;/span&gt; &amp;gt; 1.8 &lt;span class="o"&gt;]]&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="nv"&gt;$?&lt;/span&gt; -eq &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;
  &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; This java version is greater than 1.8.
  &lt;span class="k"&gt;else&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; This java version is not supported.
  &lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
  &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Java doesn&amp;#39;t exists.&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;자바 실행 파일 위치를 찾아서 하는 bash script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;which java&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
  &lt;span class="nv"&gt;JAVA_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;whereis java&lt;span class="k"&gt;)&lt;/span&gt;
  &lt;span class="nv"&gt;JAVA_VERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;java -version 2&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; awk -F&lt;span class="s1"&gt;&amp;#39;&amp;quot;&amp;#39;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/version/ {print $2}&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; awk -F&lt;span class="s1"&gt;&amp;#39;_&amp;#39;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{print $1}&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
  &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Run using java version &lt;/span&gt;&lt;span class="nv"&gt;$JAVA_VERSION&lt;/span&gt;&lt;span class="s2"&gt; (&lt;/span&gt;&lt;span class="nv"&gt;$JAVA_PATH&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt;
  &lt;span class="o"&gt;[[&lt;/span&gt; &lt;span class="nv"&gt;$JAVA_VERSION&lt;/span&gt; &amp;gt; 1.8 &lt;span class="o"&gt;]]&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="nv"&gt;$?&lt;/span&gt; -eq &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;
  &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; This java version is greater than 1.8.
  &lt;span class="k"&gt;else&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; This java version is not supported.
  &lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
  &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Java doesn&amp;#39;t exists.&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;두 방법이 사실 거의 똑같다. 그냥 whereis 명령을 이용하느냐, JAVA_HOME 이라는 환경변수를 이용하느냐만 다르다.&lt;/p&gt;
&lt;p&gt;다른 방법으로는 sort 명령을 이용해서 가장 앞에 위치하는 녀석을 비교하는 방법도 있다.&lt;/p&gt;
&lt;p&gt;처음에는 bc 명령을 사용하려 했는데 버전은 보통 여러 개의 comma(.)로 되어있어서 bc 로 사용은 불가능했다. 사실 comma(.) 단위로 나눠서 각각 비교하는 로직 구현하면 되는데 귀찮음이 커서..&lt;/p&gt;
&lt;p&gt;이 방법은 사실 완벽한 방법은 아니다. awk 로 추출한 문자열이 x.y.z 형태라는 걸 알고 있기 때문에 가능한 방법이다. 만약 1.8 로 버전이 추출되면 제대로 동작 안할 것이다. 제대로 하려면 comma(.) 단위로 나눠서 제대로 비교하는 함수를 만들어야 겠지..&lt;/p&gt;</content><category term="bash"></category></entry><entry><title>자바에서 % 출력하기</title><link href="https://imjang57.github.io/garret/print-percent-sign-in-java.html" rel="alternate"></link><published>2016-12-30T00:00:00+09:00</published><updated>2016-12-30T00:00:00+09:00</updated><author><name>imjang57</name></author><id>tag:imjang57.github.io,2016-12-30:/garret/print-percent-sign-in-java.html</id><summary type="html">&lt;p&gt;자바에서 % 문자 출력하기&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Java String 에서 % 문자 출력하기&lt;/h1&gt;
&lt;p&gt;오늘 개발하다가 &lt;code&gt;%&lt;/code&gt; 문자가 들어가는 문자열을 처리할 일이 있었다. 처음에 아무 생각없이 아래처럼 작성했다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;str1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;test&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;str2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;format&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;string is \&amp;quot;%%s%\&amp;quot;.&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;str1&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;원하는 결과는 아래와 같이 나오는 것이었다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;string is &amp;quot;%test%&amp;quot;.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;당연히 String.format 에서 에러가 발생했다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Exception in thread &amp;quot;main&amp;quot; java.util.UnknownFormatConversionException: Conversion = &amp;#39;&amp;quot;&amp;#39;
 at java.util.Formatter.checkText(Formatter.java:2579)
 at java.util.Formatter.parse(Formatter.java:2565)
 at java.util.Formatter.format(Formatter.java:2501)
 at java.util.Formatter.format(Formatter.java:2455)
 at java.lang.String.format(String.java:2940)
 at StringTest.main(StringTest.java:7)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:497)
 at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;\&lt;/code&gt; 는 특정 &lt;em&gt;espace character&lt;/em&gt; 로 이미 약속된 문자들과 사용돼야 한다. 그런데 그 문자들 중에 &lt;code&gt;%&lt;/code&gt; 는 없다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;%&lt;/code&gt; 는 &lt;em&gt;Formatting&lt;/em&gt; 을 위한 문자로, &lt;code&gt;String.format("%d", 123);&lt;/code&gt; 과 같이 사용된다. 이 때 &lt;code&gt;%&lt;/code&gt; 자체를 출력하기 위해서도 &lt;code&gt;%&lt;/code&gt; 를 prefix 로 사용하여 &lt;code&gt;%%&lt;/code&gt; 와 같이 처리해야 한다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;str1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;test&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;str2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;format&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;string is %%%s%%.&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;str1&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;%&lt;/code&gt; 는 그냥 문자가 아니라 포맷을 지정하기 위한 포맷 지시자 (format specifier 또는 format string) 역할을 하는 특수한 문자이기 때문이다.&lt;/p&gt;
&lt;p&gt;알고 있던 거였는데.. 역시 오래동안 안쓰면 머리에서 삭제되어 버린다. ㅠㅠ&lt;/p&gt;</content></entry><entry><title>SSH Host-key identification</title><link href="https://imjang57.github.io/garret/ssh-host-key-identification.html" rel="alternate"></link><published>2016-12-30T00:00:00+09:00</published><updated>2016-12-30T00:00:00+09:00</updated><author><name>imjang57</name></author><id>tag:imjang57.github.io,2016-12-30:/garret/ssh-host-key-identification.html</id><summary type="html">&lt;p&gt;This is about SSH Host-key identification.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;SSH host key&lt;/h1&gt;
&lt;p&gt;가끔 SSH Client 로 Remote 에 있는 SSH Server 에 접속할 때 아래와 같은 에러를 볼 수 있다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;imjang57@myserver:~$ ssh administrator@192.168.0.5
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!              @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now &lt;span class="o"&gt;(&lt;/span&gt;man-in-the-middle
attack&lt;span class="o"&gt;)&lt;/span&gt;!
It is also possible that a host key has just been changed.
The fingerprint &lt;span class="k"&gt;for&lt;/span&gt; the ECDSA key sent by the remote host is
bf:e3:d3:21:c4:3c:60:cd:d9:2b:bb:a4:d1:6e:1f:df.
Please contact your system administrator.
Add correct host key in /home/imjang57/.ssh/known_hosts to get rid of
this message.
Offending ECDSA key in /home/imjang57/.ssh/known_hosts:8
  remove with: ssh-keygen -f &lt;span class="s2"&gt;&amp;quot;/home/imjang57/.ssh/known_hosts&amp;quot;&lt;/span&gt; -R
192.168.0.5
ECDSA host key &lt;span class="k"&gt;for&lt;/span&gt; 192.168.0.5 has changed and you have
requested strict checking.
Host key verification failed.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Host Key&lt;/em&gt; 가 달라서 발생하는 문제다. 자세히 말하면, 클라이언트 측에 등록된 SSH 서버의 호스트 키가 현재 접속 시도하면서 새롭게 받은 호스크 키와 달라서 발생한 문제이다.&lt;/p&gt;
&lt;p&gt;SSH 서버에서 호스트 키를 새롭게 생성했거나, 클라이언트 측에서 SSH 서버의 호스트 키를 수동으로 입력했는데 잘못입력했거나, 서버를 재설치했거나, 기타 등등의 이유로 기존에 저장된 호스트 키와 연결시도하면서 새롭게 받은 호스트 키가 다를 수 있다.&lt;/p&gt;
&lt;p&gt;이는 SSH 연결을 시도하는 서버가 정말 내가 연결하려는 서버가 맞는지를 체크할 수 있는 기능이다. HTTPS 를 사용할 때 신뢰할 수 있는 사이트인지 확인하는 것과 비슷한 이유로 제공되는 기능이다.&lt;/p&gt;
&lt;h2&gt;해결 방법&lt;/h2&gt;
&lt;p&gt;해결하는 방법은 여러개가 있다.&lt;/p&gt;
&lt;h3&gt;known_host 삭제하여 해결&lt;/h3&gt;
&lt;p&gt;보통 사용자의 홈 디렉터리에 .ssh 라는 디렉터리가 있고, 여기에 사용자를 위한 SSH 설정이나 사용자 인증을 위한 키 파일이 저장된다. 그리고 &lt;user_home&gt;/.ssh 디렉터리 밑에 known_hosts 라는 파일이 있는데 여기에 SSH 서버의 호스트 키들이 저장되어 있다. 여기서 에러가 나는 SSH 서버의 호스트 키를 삭제하면 다시 연결할 수 있다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh-keygen -f &lt;span class="s2"&gt;&amp;quot;/home/imjang57/.ssh/known_hosts&amp;quot;&lt;/span&gt; -R 192.168.0.5
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;위 명령으로 저장된 서버의 호스트 키를 삭제한 후 SSH 서버에 다시 접속하면 아래와 같이 호스트 키를 등록하냐고 물어보는 메시지가 나타난다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;imjang57@myserver:~$ ssh administrator@192.168.0.5
The authenticity of host &lt;span class="s1"&gt;&amp;#39;192.168.0.5&amp;#39;&lt;/span&gt; can&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;t be established.
ECDSA key fingerprint is
bf:e3:d3:21:c4:3c:60:cd:d9:2b:bb:a4:d1:6e:1f:df.
Are you sure you want to &lt;span class="k"&gt;continue&lt;/span&gt; connecting &lt;span class="o"&gt;(&lt;/span&gt;yes/no&lt;span class="o"&gt;)&lt;/span&gt;?
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;yes 를 입력하면 호스트 키를 &lt;user_home&gt;/.ssh/known_hosts 파일에 저장하고 SSH 접속하게 된다.&lt;/p&gt;
&lt;p&gt;만약 명령어 치는게 귀찮으면 그냥 known_hosts 파일 삭제하면 된다.&lt;/p&gt;
&lt;h3&gt;StrictHostKeyChecking 설정을 off 하여 해결&lt;/h3&gt;
&lt;p&gt;리눅스에서 ssh 설정은 보통 &lt;code&gt;/etc/ssh&lt;/code&gt; 디렉터리에 있다. &lt;code&gt;ssh_config&lt;/code&gt; 파일은 클라이언트 설정 파일, &lt;code&gt;sshd_config&lt;/code&gt; 는 서버(데몬) 설정 파일이다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ssh_config&lt;/code&gt; 파일에서 아래 내용을 찾아서 호스트 키 검사를 하지 않도록 설정하면 된다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;StrictHostKeyChecking no
&lt;span class="nv"&gt;UserKnownHostsFile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/dev/null
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;호스트 키 새로 생성하는 방법&lt;/h2&gt;
&lt;p&gt;서버를 운영하는 입장에서 서버를 추가할 때 기존 서버의 이미지를 사용해서 새로운 서버를 구성할 수 있다. 이 때 호스트 키를 새롭게 생성해야 한다.&lt;/p&gt;
&lt;p&gt;호스트 키는 보통 &lt;code&gt;/etc/ssh&lt;/code&gt; 에 저장되어 있다. &lt;em&gt;RSA&lt;/em&gt;, &lt;em&gt;DSA&lt;/em&gt;, &lt;em&gt;ECDSA&lt;/em&gt; 세 가지 종류의 키 파일들이 보통 생성되어 있다.&lt;/p&gt;
&lt;p&gt;새로 호스트 키를 생성하기 위해 아래 명령을 실행해서 세 가지 종류의 호스트 키 파일들을 생성하면 된다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo ssh-keygen -f /etc/ssh/ssh_host_rsa_key -N &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt; -t rsa
sudo ssh-keygen -f /etc/ssh/ssh_host_dsa_key -N &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt; -t dsa
sudo ssh-keygen -f /etc/ssh/ssh_host_ecdsa_key -N &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt; -t ecdsa -b 521
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;기존에 파일들이 있으면 overwrite 할 거냐고 묻는데 당연히 &lt;code&gt;y&lt;/code&gt; 를 입력하자.&lt;/p&gt;</content><category term="SSH"></category></entry></feed>